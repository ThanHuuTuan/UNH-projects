{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$IBO:\\:Intelligent\\:Bayesian\\:Optimization$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- Bayes_Opt : http://haikufactory.com/files/bayopt.pdf\n",
    "- Kat Baily : http://katbailey.github.io/post/gaussian-processes-for-dummies/\n",
    "- C. E. Rasmussen & C. K. I. Williams,  : http://www.gaussianprocess.org/\n",
    "- Kevin Murphy - Machine Learning a Probabilistic Perspective: https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayesian Optimization is a technique used to find the global maxima of functions that tend to be expensive to evaluate. \n",
    "- Bayesian Optimization is a type of **black box** optimization techniques. This means that our understanding of the objective function is limited to querying a point and getting back a response. \n",
    "    - This also means that we need to define the domain over which we are seraching for bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reason why it is called bayesian optimization is that is uses bayes rule to update your enderstanding of the underlying function. This involves taking your likelihood function, P( B|A ), multipled by your prior knowledge of P( A ) to arrive at your posterior P (A|B ). In most cases, A are your parameters and B is your data.\n",
    "\n",
    " $${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}}$$\n",
    " \n",
    "- Typically, we drop the denomiator and arrive at an approximation for the posterior. This simplifies the calculation, and the resulting maximum of this aproximate posterior has the same parameter if we divided by the normalization constant.\n",
    " \n",
    "\n",
    " $${\\displaystyle P(A\\mid B)\\approx{ {P(B\\mid A)\\,P(A)}{}}}$$\n",
    " \n",
    " - Next, we attempt to maximize this posterior function which will in turn find the maximum of the underlying function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Gaussian\\:Prior$$\n",
    "<img src=\"images/gaussian_process.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://katbailey.github.io/images/kernel_cookbook.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A gaussian process is a type of model where observations occur in a continuous domain. For each point from a gaussian process, there is a corresponding normally distributed random variable. One easy way to think about Gaussian Process is as a distribution of distributions. The distribution of a gaussian process is the joint distribution of all of the points in domain.\n",
    "\n",
    ">- For every point in the domain, the relationship between points is defined by a kernel or covariance kernel. There are a multitude of kernels which in turn make assumptions about how each of the points are related to eachother.\n",
    "    - **RBF (squarred exponential kernel):** It is universal, and you can integrate it against most functions that you need to. Every function in its prior has infinitely many derivatives. It also has only two parameters.\n",
    "    - **Quadratic**: It is a good choice for smooth functions.\n",
    "    - **Brownian motion**: It is the integral of a white noise Gaussian process.  It is not stationary, but it has stationary increments.\n",
    "    \n",
    "- Info on kernels: http://www.wikiwand.com/en/Gaussian_process\n",
    "    \n",
    "- Typically, a gaussian process is have a zero mean prior which means that the entire model is defined only by the kernel (covariance function). Therefore, you can write the model as shown below where **m** is the mean (or mean vector) and **K** is the covariance (or covariance matrix).\n",
    "$$ X=GP(m,K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$Kernels$$\n",
    "<img src =\"images/gaussian_process_kernels.png\" height = \"500\" width =\"750\">\n",
    "$$Left\\:is\\:a \\:squared \\:exponential\\: kernel.\\: Middle\\: is \\:Brownian \\:Motion.\\: Right\\: is \\:quadratic.$$\n",
    "http://www.wikiwand.com/en/Gaussian_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- A gaussian process is a non-parametric machine learning approach to learn an underlying function. For each test point, the kernel (or covariance function) is used to predict the resulting value. This resulting value has two parts, a mean and a variance which together form a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$Each \\:point\\: from\\: a\\: Gaussian\\: Process\\: has\\: a\\: mean\\: and\\: variance\\: shown\\: below$$\n",
    "<img src =\"images/gaussian_process_point_normal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    ">## Setup\n",
    "- 1) Pick the number of steps that you want to search\n",
    "- 2) Define the testing domain\n",
    "- 3) Define the kernel\n",
    "- 4) Choose and acquisition function\n",
    "- 5) Maximize the objective function\n",
    "- 6) Rinse and repeat until you reach the number of steps you wanted to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "stats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
